#!/usr/bin/env python3
"""
CV Intelligence - BERT NER & Candidate Recommendation
PhiÃªn báº£n Ä‘Æ¡n giáº£n khÃ´ng cáº§n Flask, cháº¡y trá»±c tiáº¿p tá»« command line
"""
import os
import json
import sys
from pathlib import Path
from typing import List, Dict, Any

# Kiá»ƒm tra dataset
def check_dataset():
    dataset_dir = Path("Dataset/data/data")
    if not dataset_dir.exists():
        print("âŒ KhÃ´ng tÃ¬m tháº¥y dataset táº¡i:", dataset_dir)
        return False
    
    categories = [d for d in dataset_dir.iterdir() if d.is_dir()]
    print(f"âœ… TÃ¬m tháº¥y {len(categories)} danh má»¥c nghá» nghiá»‡p:")
    
    total_pdfs = 0
    for cat in categories[:10]:  # Hiá»ƒn thá»‹ 10 danh má»¥c Ä‘áº§u
        pdfs = list(cat.glob("*.pdf"))
        total_pdfs += len(pdfs)
        print(f"  ğŸ“ {cat.name}: {len(pdfs)} CVs")
    
    print(f"ğŸ“Š Tá»•ng cá»™ng: {total_pdfs} CVs")
    return True

# TrÃ­ch xuáº¥t text tá»« PDF (cáº§n cÃ i pdfminer)
def extract_pdf_text(pdf_path: str) -> str:
    try:
        from pdfminer.high_level import extract_text
        return extract_text(pdf_path)
    except ImportError:
        print("âŒ Cáº§n cÃ i pdfminer: pip install pdfminer.six")
        return ""
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c PDF {pdf_path}: {e}")
        return ""

# BERT NER (cáº§n cÃ i transformers)
def extract_entities(text: str) -> List[Dict[str, Any]]:
    try:
        from transformers import pipeline
        ner = pipeline("ner", model="dslim/bert-base-NER", aggregation_strategy="simple")
        results = ner(text)
        
        entities = []
        for r in results:
            entities.append({
                "text": r.get("word", ""),
                "label": r.get("entity_group", ""),
                "score": float(r.get("score", 0.0))
            })
        return entities
    except ImportError:
        print("âŒ Cáº§n cÃ i transformers: pip install transformers torch")
        return []
    except Exception as e:
        print(f"âŒ Lá»—i NER: {e}")
        return []

# Sentence-BERT embeddings (cáº§n cÃ i sentence-transformers)
def get_embeddings(texts: List[str]) -> List[List[float]]:
    try:
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        embeddings = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)
        return embeddings.tolist()
    except ImportError:
        print("âŒ Cáº§n cÃ i sentence-transformers: pip install sentence-transformers")
        return []
    except Exception as e:
        print(f"âŒ Lá»—i embeddings: {e}")
        return []

# TÃ¬m CV tÆ°Æ¡ng tá»±
def find_similar_cvs(query_text: str, dataset_dir: str, top_k: int = 5) -> List[Dict[str, Any]]:
    print(f"ğŸ” TÃ¬m kiáº¿m CV tÆ°Æ¡ng tá»± vá»›i: '{query_text[:50]}...'")
    
    # Láº¥y danh sÃ¡ch PDFs
    pdfs = []
    for cat_dir in Path(dataset_dir).iterdir():
        if cat_dir.is_dir():
            for pdf in cat_dir.glob("*.pdf"):
                pdfs.append((str(pdf), cat_dir.name))
    
    if not pdfs:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y PDF nÃ o")
        return []
    
    print(f"ğŸ“š Äang xá»­ lÃ½ {len(pdfs)} CVs...")
    
    # TrÃ­ch xuáº¥t text tá»« má»™t sá»‘ PDF máº«u (Ä‘á»ƒ test)
    sample_pdfs = pdfs[:10]  # Chá»‰ láº¥y 10 PDF Ä‘áº§u Ä‘á»ƒ test
    texts = []
    valid_pdfs = []
    
    for pdf_path, category in sample_pdfs:
        text = extract_pdf_text(pdf_path)
        if text.strip():
            texts.append(text)
            valid_pdfs.append((pdf_path, category))
    
    if not texts:
        print("âŒ KhÃ´ng trÃ­ch xuáº¥t Ä‘Æ°á»£c text tá»« PDF nÃ o")
        return []
    
    # Táº¡o embeddings
    print("ğŸ§  Táº¡o embeddings...")
    embeddings = get_embeddings([query_text] + texts)
    
    if not embeddings:
        print("âŒ KhÃ´ng táº¡o Ä‘Æ°á»£c embeddings")
        return []
    
    # TÃ­nh similarity (cosine)
    query_emb = embeddings[0]
    results = []
    
    for i, (pdf_path, category) in enumerate(valid_pdfs):
        doc_emb = embeddings[i + 1]
        # Cosine similarity = dot product (vÃ¬ Ä‘Ã£ normalize)
        similarity = sum(a * b for a, b in zip(query_emb, doc_emb))
        results.append({
            "path": pdf_path,
            "category": category,
            "similarity": similarity
        })
    
    # Sáº¯p xáº¿p theo similarity
    results.sort(key=lambda x: x["similarity"], reverse=True)
    return results[:top_k]

def main():
    print("ğŸš€ CV Intelligence - BERT NER & Candidate Recommendation")
    print("=" * 60)
    
    # Kiá»ƒm tra dataset
    if not check_dataset():
        return
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ MENU:")
    print("1. Kiá»ƒm tra dataset")
    print("2. TrÃ­ch xuáº¥t thá»±c thá»ƒ tá»« text")
    print("3. TÃ¬m CV tÆ°Æ¡ng tá»±")
    print("4. ThoÃ¡t")
    
    while True:
        try:
            choice = input("\nChá»n chá»©c nÄƒng (1-4): ").strip()
            
            if choice == "1":
                check_dataset()
                
            elif choice == "2":
                text = input("Nháº­p text CV (hoáº·c Enter Ä‘á»ƒ dÃ¹ng text máº«u): ").strip()
                if not text:
                    text = "John Smith is a Software Engineer with 5 years of experience in Python and JavaScript. He worked at Google and Microsoft. Contact: john@email.com"
                    print(f"Sá»­ dá»¥ng text máº«u: {text[:50]}...")
                
                print("ğŸ” Äang trÃ­ch xuáº¥t thá»±c thá»ƒ...")
                entities = extract_entities(text)
                
                if entities:
                    print("âœ… Thá»±c thá»ƒ Ä‘Æ°á»£c trÃ­ch xuáº¥t:")
                    for entity in entities:
                        print(f"  ğŸ“ {entity['text']} -> {entity['label']} (Ä‘á»™ tin cáº­y: {entity['score']:.2f})")
                else:
                    print("âŒ KhÃ´ng trÃ­ch xuáº¥t Ä‘Æ°á»£c thá»±c thá»ƒ nÃ o")
                    
            elif choice == "3":
                query = input("Nháº­p mÃ´ táº£ cÃ´ng viá»‡c/CV Ä‘á»ƒ tÃ¬m kiáº¿m: ").strip()
                if not query:
                    query = "Software Engineer Python JavaScript"
                    print(f"Sá»­ dá»¥ng query máº«u: {query}")
                
                results = find_similar_cvs(query, "Dataset/data/data", top_k=5)
                
                if results:
                    print("ğŸ¯ CV tÆ°Æ¡ng tá»± nháº¥t:")
                    for i, result in enumerate(results, 1):
                        print(f"  {i}. {result['category']} (Ä‘á»™ tÆ°Æ¡ng tá»±: {result['similarity']:.3f})")
                        print(f"     ğŸ“„ {result['path']}")
                else:
                    print("âŒ KhÃ´ng tÃ¬m tháº¥y CV tÆ°Æ¡ng tá»±")
                    
            elif choice == "4":
                print("ğŸ‘‹ Táº¡m biá»‡t!")
                break
                
            else:
                print("âŒ Lá»±a chá»n khÃ´ng há»£p lá»‡")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Táº¡m biá»‡t!")
            break
        except Exception as e:
            print(f"âŒ Lá»—i: {e}")

if __name__ == "__main__":
    main()
